{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ygon7645_COMP5046_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "MGHoy6KpQDfZ"
      },
      "cell_type": "markdown",
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qTf21j_oQIiD"
      },
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iXbQohXLKSgO"
      },
      "cell_type": "markdown",
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "34DVNKgqQY21"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7cWUxAQrGlq6"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "sCs6uUnKXj7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U7C4snIcNl22",
        "outputId": "8c6ea817-9de8-4c06-b65b-0c949d1b3f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "import json\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "IsaB3YSBWIFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id_comic = '12c7PwE2BKsrCIBx_Ve3qGkn_toAJ0IMg'\n",
        "downloaded = drive.CreateFile({'id':id_comic}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')\n",
        "#https://drive.google.com/file/d/12c7PwE2BKsrCIBx_Ve3qGkn_toAJ0IMg/view?usp=sharing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZoLxMBCX0P1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id_friend = '11Eee3u_yfy4MT5UFTts-p86gocMXiOco'\n",
        "downloaded = drive.CreateFile({'id':id_friend}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')\n",
        "#https://drive.google.com/file/d/11Eee3u_yfy4MT5UFTts-p86gocMXiOco/view?usp=sharing\n",
        "\n",
        "id_professional = '12Dn7am7lOZrzCHYsEJgg-xzrQByc-llC'\n",
        "downloaded = drive.CreateFile({'id':id_professional}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')\n",
        "#https://drive.google.com/file/d/12Dn7am7lOZrzCHYsEJgg-xzrQByc-llC/view?usp=sharing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4Tt56l4cAlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from io import open\n",
        "\n",
        "\n",
        "data_com = open(\"qna_chitchat_the_comic.tsv\", \"r\", encoding=\"utf-8\")\n",
        "data_fri = open(\"qna_chitchat_the_friend.tsv\", \"r\", encoding=\"utf-8\")\n",
        "data_pro = open(\"qna_chitchat_the_professional.tsv\", \"r\", encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "l9gBSgBCQh24"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8RdKI8E2KRwe"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
      ]
    },
    {
      "metadata": {
        "id": "1ld0CmLG4bJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here are four techniques conducted: punctuations removal, decapitalisation, tokenisation and stopwords removal.\n",
        "\n",
        "1.  Punctuations removal\n",
        "     \n",
        "     Removing punctuations in the text is a cleaning process of the raw data.  Here, 're' is imported to realize the process. As we know that FastText with CBOW will generate great dimensional vectors for each word. Punctuations and non-English characters are not available. In particular, when we fetch a page from a website, a lot labels of html were loaded together. As a result, the cleaning process is necessary.\n",
        "2.  Decapitalisation\n",
        "\n",
        "     According to the characteristics of English, decapitalisation is useful to reduce the number of training words. More specifically, two words with the same spelling and semantic but one is uppercase the other is lowercase, they will be treated as different words in FastText. Decapitalisation is used for reducing repitition of words and improving calculating efficiency.\n",
        "3.   Tokenisation\n",
        "    \n",
        "      For questions in the raw data, we need to tokenize each sentence as independent words. But for the answers, we treat each single answer as a token.\n",
        "4.   Stopwords removal\n",
        "\n",
        "      Stopwords are unnecessary in a sentence. Removing them usually has no effect on understanding the meaning of a whole sentence. In a text, there will be a large number of function words, pronouns or verbs and nouns without specific meanings. These words are of no help to the text analysis, so we hope to remove these stopwords.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WpBxhu6Ccqom",
        "colab_type": "code",
        "outputId": "45c9142d-67ba-4ab8-fc8f-30c65905877e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "WsZORMRddSVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_com = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n",
        "df_fri = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df_pro = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LIu_lkJwQ55g"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "daDvAftceIvr"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lbzm-NWBTmM-"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
      ]
    },
    {
      "metadata": {
        "id": "F8-O47g8ulOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here I would like to use FastText with CBOW.\n",
        "\n",
        "Firstly, FastText is an open source word vector computing and text categorization tool developed by facebook. As there are many popular models to learn such representations ignore the morphology of words, by assigning a\n",
        "distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. FastText uses a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. This method is fast, allowing to trainmodels on large corpus quickly. (*Piotr Bojanowski∗and Edouard Grave∗and Armand Joulin and Tomas Mikolov from Facebook AI Research, 15 Jul 2016*). In a word, the value of fastText is that it provides a more readable and modular implementation of word2vec.\n",
        "\n",
        "Besides, in the CBOW method, the surrounding words are used to predict the center words, making use of the prediction result of the center words. The gradient desent method is used to constantly adjust the vector of the surrounding words. When the training is completed, each word will be used as the central word, and the word vectors of the surrounding words will be adjusted, so as to obtain the word vectors of all the words in the whole text. While, Skip-gram uses the central word to predict the words around it. In SkipGram, gradient decent is used to adjust the word vectors of the center words constantly to predict results of the surrounding words. Finally, the word vectors of all words in the text are obtained.\n",
        "\n",
        "In other words, CBOW is inferring the target word from the original statement; Skip-Gram, on the other hand, it predicts the original sentence from the target word. CBOW is suitable for small databases, while Skip-Gram performs better in large corpus.\n",
        "\n",
        "As I used Microsoft BotBuilder chat datasets for word embeddings. The dataset is not that large. Besides, There are a large number of high-frequency words in the corpus. Here, CBOW is more efficient because it takes much less predicted time than SkipGram."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "it6I1_K7HTub"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4Op66omXKVHa"
      },
      "cell_type": "markdown",
      "source": [
        "Here I still use Microsoft BotBuilder chat datasets. \n",
        "\n",
        "The Cornell Movie Dialog Corpus is complex and the amount of its data is large. Moreover, the format of Q&A is not uniform. As I use FastText with CBOW, which is not suitable for very complex data. So Microsoft BotBuilder chat datasets are better choices."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QLjf_pm9NiA8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "datacom=df_com\n",
        "datafri=df_fri\n",
        "datapro=df_pro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GXgFpxIgl-_G"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qJrVHGYSmYMg"
      },
      "cell_type": "markdown",
      "source": [
        "The data preprocessing is the same as that in 1.2."
      ]
    },
    {
      "metadata": {
        "id": "Ue6CKgz1g1TH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "\n",
        "def makesentence(data):\n",
        "  parse_text=[]\n",
        "  for i in range(data.shape[0]):    \n",
        "      s1=[data['Question'][i]]\n",
        "      s2=data['Answer'][i]\n",
        "      parse_text.append([s1,s2])\n",
        "      \n",
        "  return parse_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxAkhW-4gaoh",
        "colab_type": "code",
        "outputId": "e302e344-5945-4520-e4dd-31e8754181b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def make_tokenlist(text): \n",
        "  for i in text:\n",
        "      content_text = re.sub(r'\\([^)]*\\)', '', i[0][0])\n",
        "\n",
        "      normalized_text = \"\"\n",
        "      for string in content_text:####\n",
        "          tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "          #normalized_text.append(tokens)\n",
        "          normalized_text += tokens\n",
        "      i[0][0]= normalized_text.split()\n",
        "  new_text=[]\n",
        "  for i in text:\n",
        "    new_i=[]\n",
        "    new_str=\"\"\n",
        "    for j in i[0][0]:\n",
        "      new_i.append(j)\n",
        "    new_i.append(i[1])\n",
        "    new_text.append(new_i)\n",
        "  filtered_sentence = [w for w in new_text if not w in stopwords.words()]\n",
        "  return filtered_sentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qhAgWf_AmbZ8"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AJ8rU7JbiBVS"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "O7Kejin__J3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "size: length of the word vector (defaulted=100)\n",
        "\n",
        "window: The size of the context window (defaulted=5)\n",
        "\n",
        "min_count: It is used to discard the words that appear less than min_count times in the training corpus. As we have paddings in datasets, it should be equal to 1.\n",
        "\n",
        "workers: The number of worker threads to train the model (=faster training with multicore machines).\n",
        "\n",
        "sg: Training algorithm --- CBOW if sg=0, otherwise skip-gram.\n"
      ]
    },
    {
      "metadata": {
        "id": "AUo3fWklgRhE",
        "colab_type": "code",
        "outputId": "a50b4763-d655-4496-d936-4faa506d71fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "textcom=makesentence(datacom)\n",
        "textfri=makesentence(datafri)\n",
        "textpro=makesentence(datapro)\n",
        "\n",
        "normalized_textcom=make_tokenlist(textcom)\n",
        "normalized_textfri=make_tokenlist(textfri)\n",
        "normalized_textpro=make_tokenlist(textpro)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LNys5HOdISK-"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "JE81Uwtv6thJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ft_cbow_model_com=FastText(normalized_textcom, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "ft_cbow_model_fri=FastText(normalized_textfri, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "ft_cbow_model_pro=FastText(normalized_textpro, size=100, window=5, min_count=1, workers=4, sg=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uMCv3YI1IfUo"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "BOR1EUNiltlg",
        "colab_type": "code",
        "outputId": "614082aa-034f-40fd-ab3b-fef94b55cf9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ft_cbow_model_com\n",
        "ft_cbow_model_fri\n",
        "ft_cbow_model_pro"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.fasttext.FastText at 0x7fe71bf60470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "RKlTxqb1Oo1P",
        "colab_type": "code",
        "outputId": "ea6d3c32-2daa-41ff-c940-fdfcf8dfcf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "path = get_tmpfile(\"word2vec.ft_cbow_model_com\")\n",
        "ft_cbow_model_com.save(\"word2vec.ft_cbow_model_com\")\n",
        "  \n",
        "uploaded = drive.CreateFile({'title': 'models_comic.ckpt'})\n",
        "uploaded.SetContentFile('word2vec.ft_cbow_model_com')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1dc-R1bQeWdnQ3VJ7oxqmy_Rtz768MhJH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XZPkIb-RRZ29",
        "colab_type": "code",
        "outputId": "e46a5a8e-be6f-4175-9401-39fac77ef5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = get_tmpfile(\"word2vec.ft_cbow_model_fri\")\n",
        "ft_cbow_model_fri.save(\"word2vec.ft_cbow_model_fri\")\n",
        "  \n",
        "uploaded1 = drive.CreateFile({'title': 'models_friend.ckpt'})\n",
        "uploaded1.SetContentFile('word2vec.ft_cbow_model_fri')\n",
        "uploaded1.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded1.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1xRbzt3YklqID-miTMeosJZKIkcRTz3cl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NBIlM3LRvBt",
        "colab_type": "code",
        "outputId": "ba270d6c-f834-4a1a-e1ac-1b8f3d86e5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = get_tmpfile(\"word2vec.ft_cbow_model_pro\")\n",
        "ft_cbow_model_pro.save(\"word2vec.ft_cbow_model_pro\")\n",
        "  \n",
        "uploaded2 = drive.CreateFile({'title': 'models_professional.ckpt'})\n",
        "uploaded2.SetContentFile('word2vec.ft_cbow_model_pro')\n",
        "uploaded2.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded2.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1NkRwrD4_5Tt_bPzj6yMTSJIAatvwL9Ti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Yn16xrDrIs8B"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-IebpYFsIvgh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "#id_comic_model = '1qO8UAV33BX6y-O48Acs_blQbUErOoCsx'\n",
        "id_comic_model = '1NgJGO9uP1whtvZR1p8LOkL8XSu49_Lew'\n",
        "downloaded = drive.CreateFile({'id':id_comic_model}) \n",
        "downloaded.GetContentFile('models_comic.ckpt')\n",
        "\n",
        "#id_friend_model = '1oAjwBZhOOM-j27EcUFF-RCzRlJxNdStB'\n",
        "id_friend_model = '1W3tEtRNsc0HThNgKQh1pRHssEdWNa-KZ'\n",
        "downloaded1 = drive.CreateFile({'id':id_friend_model}) \n",
        "downloaded1.GetContentFile('models_friend.ckpt')\n",
        "\n",
        "#id_professional_model = '16kkEL6fg3Eg4Nvw4phkyqNSXEaZ43blA'\n",
        "id_professional_model ='1NkRwrD4_5Tt_bPzj6yMTSJIAatvwL9Ti'\n",
        "downloaded2 = drive.CreateFile({'id':id_professional_model}) \n",
        "downloaded2.GetContentFile('models_professional.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tlCeWT8eeLnd"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fwA-NN3EJ4Ig"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UAMJrxx-iOVn"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "sU8sHAaxFzLV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "max_input_words_amount: \n",
        "\n",
        "Here, we use max(len(tokenized_q, max_input_words_amount) to get max_input_words_amount, which will be applied to the next N-1 seq2seq model. It is used for getting paddings to let the input length consistent.\n",
        "\n",
        "num_dic, dic_len: \n",
        "\n",
        "They are used for building the targets of the seq2seq model. They are applied to two-dimensional array of targets.\n",
        "\n",
        "seq_data:\n",
        "\n",
        "It is an important list for getting word vectors to make batches of the seq2seq model.\n",
        "\n",
        "unique_words: \n",
        "\n",
        "A list for storing nonredundant preprocessed words in order to get paddings.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g7PKX1gIePA2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "\n",
        "model_com = FastText.load(\"models_comic.ckpt\")\n",
        "model_fri = FastText.load(\"models_friend.ckpt\")\n",
        "model_pro = FastText.load(\"models_professional.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DpYCL17JKZxl"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R204UIyDKhZ4"
      },
      "cell_type": "markdown",
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "metadata": {
        "id": "_8L4Zzu5KPdR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "learning_rate :\n",
        "\n",
        "The learning rate determines the speed at which the parameter is moved to the optimal value. If the learning rate is too high, it is likely to exceed the optimal value; if the learning rate is too low, the optimization efficiency may be too low and the convergence time will be extremely long. Here we set it as 0.002.\n",
        " \n",
        " n_hidden :\n",
        " \n",
        " If the number of hidden neurons is too small, the network can not have the necessary learning ability and information processing ability. On the contrary, too much neurons will not only greatly increase the complexity of the network structure, but also make the network fall into local minima more easily in the learning process, and make the learning speed of the network become very slow. Here n_hidden equals to 128.\n",
        "\n",
        " n_class :\n",
        " \n",
        " It varies according to different models, it equals to dic_len.\n",
        " \n",
        " n_input :\n",
        " \n",
        " It equals to 100. (length of a word vector. )\n",
        " "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "13eCtR_SLUG6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "import re\n",
        "import string\n",
        "#import pickle\n",
        "def build_seq(df,model):\n",
        "  seq_data = []\n",
        "  whole_words = []\n",
        "  max_input_words_amount = 0\n",
        "  max_output_words_amount = 1\n",
        "\n",
        "  \n",
        "  for index, row in df.iterrows():\n",
        "  \n",
        "    question = row.Question.lower()\n",
        "    answer = row.Answer\n",
        "    \n",
        "    seq_data.append([question,answer])\n",
        "    \n",
        "    # we need to tokenise question \n",
        "    for i in question:\n",
        "        if i in string.punctuation: \n",
        "            question = question.replace(i,\" \")\n",
        "    for i in question:\n",
        "        if i in string.digits:\n",
        "            question = question.replace(i,\" \")\n",
        "     \n",
        "    tokenized_q = question.split()\n",
        "    #tokenized_q=[w for w in tokenized_qes if not w in stopwords.words()]###\n",
        "    \n",
        "    # we do not need to tokenise answer (because we implement N to One model)\n",
        "    # make a list with only one element (whole sentence)\n",
        "    tokenized_a =[answer]\n",
        "    \n",
        "    ###</You need to fill here>###\n",
        "    \n",
        "    \n",
        "    # add question list and answer list (one element)\n",
        "    whole_words+=tokenized_q\n",
        "    whole_words += tokenized_a\n",
        "    #whole_words.append(tokenized_a)\n",
        "    \n",
        "    # we need to decide the maximum size of input word tokens\n",
        "    max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
        "    \n",
        "\n",
        "# we now have a vacabulary list\n",
        "  unique_words = list(set(whole_words))\n",
        "\n",
        "# adding special tokens in the vocabulary list    \n",
        "# _B_: Beginning of Sequence\n",
        "# _E_: Ending of Sequence\n",
        "# _P_: Padding of Sequence - for different size input\n",
        "# _U_: Unknown element of Sequence - for different size input\n",
        "\n",
        "  unique_words.append('_B_')\n",
        "  unique_words.append('_E_')\n",
        "  unique_words.append('_P_')\n",
        "  unique_words.append('_U_')\n",
        "\n",
        "\n",
        "  num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "\n",
        "  dic_len = len(num_dic)\n",
        "  \n",
        "  \n",
        "  #for key, value in num_dic.items():\n",
        "     ###d.iteritems: an iterator over the (key, value) items\n",
        "   # if (key!='_B_')&(key!='_E_')&(key!='_P_')&(key!='_U_'):\n",
        "    #  vec=model[key]\n",
        "     # value=vec\n",
        "  #p_num_dic=pi(num_dic) \n",
        "  #p_seq_data=pi(seq_data)\n",
        "  return num_dic,dic_len,seq_data,max_input_words_amount,unique_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yWEbGOgKzSYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_com=build_seq(datacom,model_com)\n",
        "seq_fri=build_seq(datafri,model_fri)\n",
        "seq_pro=build_seq(datapro,model_pro)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4x1xnF3W_sY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "com_dic,com_len,com_seq,com_max,com_unique=seq_com"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lT_8CSWYocSj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fri_dic,fri_len,fri_seq,fri_max,fri_unique=seq_fri"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FIXViKEcommU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pro_dic,pro_len,pro_seq,pro_max,pro_unique=seq_pro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OH_Yjb-QiGlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_target_a(sentence,num_dic):\n",
        "    tokenized_sentence = [sentence]\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        \n",
        "        if token in num_dic:\n",
        "            ids.append(num_dic[token])\n",
        "        else:\n",
        "            ids.append(num_dic['_U_'])\n",
        "              \n",
        "\n",
        "    return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4JFLuEX6AlN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "def get_vectors_q(sentence,model,num_dic,max_):\n",
        "    \n",
        "    # tokenise the sentence\n",
        "    \n",
        "    max_input_words_amount = max_\n",
        "    sen = sentence.lower()\n",
        "    for i in sen:\n",
        "        if i in string.punctuation: \n",
        "            sen = sen.replace(i,\" \")\n",
        "    for i in sen:\n",
        "        if i in string.digits:\n",
        "            sen = sen.replace(i,\" \")\n",
        "            \n",
        "    #filtered_sentence = [w for w in tokens if not w in stopwords.words()]\n",
        "    tokens =  sen.split()\n",
        "    tokenized_sentence = [w for w in tokens if not w in stopwords.words()]\n",
        "     ###stop words\n",
        "    \n",
        "   \n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        \n",
        "        tokenized_sentence.append('_?_')\n",
        "               \n",
        "    \n",
        "    data = tokens_to_ids(tokenized_sentence,model,num_dic)\n",
        "         \n",
        "    \n",
        "        \n",
        "    return data\n",
        "\n",
        "def get_vectors_a(sentence,model,num_dic):    \n",
        "    tokenized_sentence = [sentence]\n",
        "    data = tokens_to_ids(tokenized_sentence,model,num_dic)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "# convert tokens to index\n",
        "def tokens_to_ids(tokenized_sentence,model,num_dic):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "      try:\n",
        "        #if token in num_dic:\n",
        "          ids.append(model[token])\n",
        "      except KeyError as e:\n",
        "        #else:\n",
        "          ids.append(np.random.rand(100))\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(seq_data,model,num_dic,dic,max_):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:  \n",
        "        \n",
        "        # Input for encoder cell, convert question to vector\n",
        " \n",
        "        input_data = get_vectors_q(seq[0],model,num_dic,max_) \n",
        "      \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "       \n",
        "        output_data = [np.zeros(100)]#[np.random.rand(100)]\n",
        "     \n",
        "        output_data += get_vectors_a(seq[1],model,num_dic)\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "       \n",
        "        target = get_target_a(seq[1],dic)\n",
        "        target.append(dic['_E_'])\n",
        "        #target = get_vectors_a(seq[1],model,num_dic)\n",
        "        #target.append(np.array([0] * 100))\n",
        "        \n",
        "        array_input=np.array(input_data)\n",
        "        array_output=np.array(output_data)\n",
        "        # Convert each token vector to one-hot encode data\n",
        "        input_batch.append(array_input)\n",
        "        output_batch.append(array_output)\n",
        "        target_batch.append(target)\n",
        "        \n",
        "       \n",
        "\n",
        "    return input_batch, output_batch, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZOOPeVkVCve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "com_key=[]\n",
        "for key in com_dic:\n",
        "  com_key.append(key)\n",
        "  #p_com_key=pi(com_key)\n",
        "fri_key=[]\n",
        "for key in fri_dic:\n",
        "  fri_key.append(key)\n",
        "  #p_fri_key=pi(fri_key)\n",
        "pro_key=[]\n",
        "for key in pro_dic:\n",
        "  pro_key.append(key)\n",
        "  #p_pro_key=pi(pro_key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wx2E_c8N-mwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tf.reset_default_graph()\n",
        "def build(com_len):\n",
        "    learning_rate = 0.002\n",
        "    n_hidden = 128\n",
        "\n",
        "    n_class = com_len\n",
        "    n_input = 100\n",
        "\n",
        "    ### Neural Network Model\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # encoder/decoder shape = [batch size, time steps, input size]\n",
        "    enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "    dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "\n",
        "    # target shape = [batch size, time steps]\n",
        "    targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "    # Encoder Cell\n",
        "    with tf.variable_scope('encode'):\n",
        "        enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "        enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "        outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                                dtype=tf.float32)\n",
        "    # Decoder Cell\n",
        "    with tf.variable_scope('decode'):\n",
        "        dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "        dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "        # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "        outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                                  initial_state=enc_states,\n",
        "                                                  dtype=tf.float32)\n",
        "\n",
        "    model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "    cost = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=model, labels=targets))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        " \n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    return sess,model,cost,optimizer,enc_input,dec_input,targets\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6BaOiaGRLW7R"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "4qzflikt-Ov9",
        "colab_type": "code",
        "outputId": "38f0f5b4-f2b5-43d4-c286-df5dd853aa95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sess,model,cost,optimizer,enc_input,dec_input,targets=build(com_len)\n",
        "\n",
        "input_batch, output_batch, target_batch = make_batch(com_seq,model_com,com_key,com_dic,com_max)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-7a753dd2757d>:21: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-25-7a753dd2757d>:25: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-25-7a753dd2757d>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AgwKbvWnxKSY",
        "colab_type": "code",
        "outputId": "d2a39e1a-bd0f-4cf9-f242-80eec13ac7f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "saver_com = tf.train.Saver()\n",
        "total_epoch = 5000\n",
        "\n",
        "#input_batch=lo(p_input_batch)\n",
        "#output_batch=lo(p_output_batch)\n",
        "#target_batch=lo(p_target_batch)\n",
        "for epoch in range(total_epoch):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={enc_input: input_batch,\n",
        "                                  dec_input: output_batch,\n",
        "                                  targets: target_batch})\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1),\n",
        "              'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('Epoch:', '%04d' % (epoch + 1),\n",
        "      'cost =', '{:.6f}'.format(loss))\n",
        "print('Training completed')\n",
        "save_path_com = saver_com.save(sess,\"save/model_com.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 6.380381\n",
            "Epoch: 0101 cost = 2.236198\n",
            "Epoch: 0201 cost = 1.485604\n",
            "Epoch: 0301 cost = 0.472460\n",
            "Epoch: 0401 cost = 0.119365\n",
            "Epoch: 0501 cost = 0.045360\n",
            "Epoch: 0601 cost = 0.026266\n",
            "Epoch: 0701 cost = 0.015445\n",
            "Epoch: 0801 cost = 0.012115\n",
            "Epoch: 0901 cost = 0.008448\n",
            "Epoch: 1001 cost = 0.009590\n",
            "Epoch: 1101 cost = 0.005430\n",
            "Epoch: 1201 cost = 0.004650\n",
            "Epoch: 1301 cost = 0.005830\n",
            "Epoch: 1401 cost = 0.003846\n",
            "Epoch: 1501 cost = 0.003131\n",
            "Epoch: 1601 cost = 0.002933\n",
            "Epoch: 1701 cost = 0.002351\n",
            "Epoch: 1801 cost = 0.002114\n",
            "Epoch: 1901 cost = 0.001981\n",
            "Epoch: 2001 cost = 2.467515\n",
            "Epoch: 2101 cost = 0.010895\n",
            "Epoch: 2201 cost = 0.005530\n",
            "Epoch: 2301 cost = 0.004488\n",
            "Epoch: 2401 cost = 0.003840\n",
            "Epoch: 2501 cost = 0.004113\n",
            "Epoch: 2601 cost = 0.001919\n",
            "Epoch: 2701 cost = 0.001927\n",
            "Epoch: 2801 cost = 0.001649\n",
            "Epoch: 2901 cost = 0.002074\n",
            "Epoch: 3001 cost = 0.001647\n",
            "Epoch: 3101 cost = 0.001216\n",
            "Epoch: 3201 cost = 0.000940\n",
            "Epoch: 3301 cost = 0.001538\n",
            "Epoch: 3401 cost = 0.000852\n",
            "Epoch: 3501 cost = 0.001093\n",
            "Epoch: 3601 cost = 0.001116\n",
            "Epoch: 3701 cost = 0.001920\n",
            "Epoch: 3801 cost = 0.000824\n",
            "Epoch: 3901 cost = 0.000607\n",
            "Epoch: 4001 cost = 0.000901\n",
            "Epoch: 4101 cost = 0.000494\n",
            "Epoch: 4201 cost = 0.000513\n",
            "Epoch: 4301 cost = 0.000809\n",
            "Epoch: 4401 cost = 0.000963\n",
            "Epoch: 4501 cost = 0.000382\n",
            "Epoch: 4601 cost = 0.000508\n",
            "Epoch: 4701 cost = 0.000514\n",
            "Epoch: 4801 cost = 0.000315\n",
            "Epoch: 4901 cost = 0.000423\n",
            "Epoch: 5000 cost = 0.000415\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QEtGJoXjBFaE",
        "colab_type": "code",
        "outputId": "3b9d85a7-5e94-4a30-e921-6783c42e25c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "sess,model,cost,optimizer,enc_input,dec_input,targets=build(fri_len)\n",
        "\n",
        "n_class=fri_len\n",
        "input_batch2, output_batch2, target_batch2 = make_batch(fri_seq,model_fri,fri_key,fri_dic,fri_max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lVQnUSX1LZ6C",
        "outputId": "796eb1cd-35a1-4723-ff49-bb1ecefc1c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "saver_fri = tf.train.Saver() \n",
        "total_epoch1=5000\n",
        "for epoch in range(total_epoch1):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={enc_input: input_batch2,\n",
        "                                  dec_input: output_batch2,\n",
        "                                  targets: target_batch2})\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1),\n",
        "              'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('Epoch:', '%04d' % (epoch + 1),\n",
        "      'cost =', '{:.6f}'.format(loss))\n",
        "print('Training completed')\n",
        "save_path_fri = saver_fri.save(sess,\"save/model_fri.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 6.414020\n",
            "Epoch: 0101 cost = 2.251807\n",
            "Epoch: 0201 cost = 1.471490\n",
            "Epoch: 0301 cost = 0.465624\n",
            "Epoch: 0401 cost = 0.109619\n",
            "Epoch: 0501 cost = 0.046409\n",
            "Epoch: 0601 cost = 0.026690\n",
            "Epoch: 0701 cost = 0.018336\n",
            "Epoch: 0801 cost = 0.012088\n",
            "Epoch: 0901 cost = 0.009323\n",
            "Epoch: 1001 cost = 0.005849\n",
            "Epoch: 1101 cost = 0.005402\n",
            "Epoch: 1201 cost = 0.005102\n",
            "Epoch: 1301 cost = 0.004303\n",
            "Epoch: 1401 cost = 0.003911\n",
            "Epoch: 1501 cost = 0.003239\n",
            "Epoch: 1601 cost = 0.002584\n",
            "Epoch: 1701 cost = 0.472208\n",
            "Epoch: 1801 cost = 0.033196\n",
            "Epoch: 1901 cost = 0.017856\n",
            "Epoch: 2001 cost = 0.010487\n",
            "Epoch: 2101 cost = 0.008392\n",
            "Epoch: 2201 cost = 0.006193\n",
            "Epoch: 2301 cost = 0.005484\n",
            "Epoch: 2401 cost = 0.003149\n",
            "Epoch: 2501 cost = 0.003948\n",
            "Epoch: 2601 cost = 0.003406\n",
            "Epoch: 2701 cost = 0.002515\n",
            "Epoch: 2801 cost = 0.002670\n",
            "Epoch: 2901 cost = 0.002060\n",
            "Epoch: 3001 cost = 0.002138\n",
            "Epoch: 3101 cost = 0.001803\n",
            "Epoch: 3201 cost = 0.001496\n",
            "Epoch: 3301 cost = 0.001495\n",
            "Epoch: 3401 cost = 0.001756\n",
            "Epoch: 3501 cost = 0.001067\n",
            "Epoch: 3601 cost = 0.001058\n",
            "Epoch: 3701 cost = 0.001195\n",
            "Epoch: 3801 cost = 0.000960\n",
            "Epoch: 3901 cost = 0.000752\n",
            "Epoch: 4001 cost = 0.000748\n",
            "Epoch: 4101 cost = 0.000759\n",
            "Epoch: 4201 cost = 0.000695\n",
            "Epoch: 4301 cost = 0.000749\n",
            "Epoch: 4401 cost = 0.001256\n",
            "Epoch: 4501 cost = 0.147312\n",
            "Epoch: 4601 cost = 0.003475\n",
            "Epoch: 4701 cost = 0.003296\n",
            "Epoch: 4801 cost = 0.002184\n",
            "Epoch: 4901 cost = 0.001843\n",
            "Epoch: 5000 cost = 0.001318\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G3tn_e3jUPC-",
        "colab_type": "code",
        "outputId": "a100fec4-4f6f-4bf2-dc5d-0230d2c90c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "sess,model,cost,optimizer,enc_input,dec_input,targets=build(pro_len)\n",
        "\n",
        "n_class=pro_len\n",
        "input_batch3, output_batch3, target_batch3 = make_batch(pro_seq,model_pro,pro_key,pro_dic,pro_max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CHCMg8XDFYDK",
        "colab_type": "code",
        "outputId": "82adfeba-0f1f-4332-a63e-2de5e6cccc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "total_epoch2=5000\n",
        "saver_pro = tf.train.Saver()\n",
        "for epoch in range(total_epoch2):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={enc_input: input_batch3,\n",
        "                                  dec_input: output_batch3,\n",
        "                                  targets: target_batch3})\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1),\n",
        "              'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('Epoch:', '%04d' % (epoch + 1),\n",
        "      'cost =', '{:.6f}'.format(loss))\n",
        "print('Training completed')\n",
        "save_path_pro = saver_pro.save(sess,\"save/model_pro.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 6.474985\n",
            "Epoch: 0101 cost = 2.236373\n",
            "Epoch: 0201 cost = 1.463908\n",
            "Epoch: 0301 cost = 0.383279\n",
            "Epoch: 0401 cost = 0.095638\n",
            "Epoch: 0501 cost = 0.045677\n",
            "Epoch: 0601 cost = 0.027191\n",
            "Epoch: 0701 cost = 0.017265\n",
            "Epoch: 0801 cost = 0.013254\n",
            "Epoch: 0901 cost = 0.010115\n",
            "Epoch: 1001 cost = 0.008563\n",
            "Epoch: 1101 cost = 0.006029\n",
            "Epoch: 1201 cost = 0.004776\n",
            "Epoch: 1301 cost = 0.003710\n",
            "Epoch: 1401 cost = 0.003550\n",
            "Epoch: 1501 cost = 0.003348\n",
            "Epoch: 1601 cost = 0.042030\n",
            "Epoch: 1701 cost = 0.010983\n",
            "Epoch: 1801 cost = 0.008197\n",
            "Epoch: 1901 cost = 0.005574\n",
            "Epoch: 2001 cost = 0.004556\n",
            "Epoch: 2101 cost = 0.003580\n",
            "Epoch: 2201 cost = 0.003237\n",
            "Epoch: 2301 cost = 0.002678\n",
            "Epoch: 2401 cost = 0.002307\n",
            "Epoch: 2501 cost = 0.002137\n",
            "Epoch: 2601 cost = 0.001857\n",
            "Epoch: 2701 cost = 0.002017\n",
            "Epoch: 2801 cost = 0.001482\n",
            "Epoch: 2901 cost = 0.001605\n",
            "Epoch: 3001 cost = 0.001354\n",
            "Epoch: 3101 cost = 0.001144\n",
            "Epoch: 3201 cost = 0.000983\n",
            "Epoch: 3301 cost = 0.000865\n",
            "Epoch: 3401 cost = 0.001054\n",
            "Epoch: 3501 cost = 0.000999\n",
            "Epoch: 3601 cost = 0.000549\n",
            "Epoch: 3701 cost = 0.000989\n",
            "Epoch: 3801 cost = 0.000543\n",
            "Epoch: 3901 cost = 0.000526\n",
            "Epoch: 4001 cost = 0.000595\n",
            "Epoch: 4101 cost = 0.000603\n",
            "Epoch: 4201 cost = 0.000446\n",
            "Epoch: 4301 cost = 0.000394\n",
            "Epoch: 4401 cost = 0.000455\n",
            "Epoch: 4501 cost = 0.000378\n",
            "Epoch: 4601 cost = 0.001030\n",
            "Epoch: 4701 cost = 0.000403\n",
            "Epoch: 4801 cost = 0.000349\n",
            "Epoch: 4901 cost = 0.006357\n",
            "Epoch: 5000 cost = 0.003658\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-2feNpG-LZx2"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "QO_rS35FZx_3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id_c1='1ag6M15IoCAwxZIDtmWVlJbuXCsfFhwLm'\n",
        "#https://drive.google.com/open?id=1ag6M15IoCAwxZIDtmWVlJbuXCsfFhwLm\n",
        "\n",
        "id_c2='1vFj3AI35CbF9FnabapvyKa8VAvNUmCW-'\n",
        "#https://drive.google.com/open?id=1vFj3AI35CbF9FnabapvyKa8VAvNUmCW-\n",
        "\n",
        "id_c3='1RoM9q3caYO6I7UyuVtXynlEscKdqjEev'\n",
        "#https://drive.google.com/open?id=1RoM9q3caYO6I7UyuVtXynlEscKdqjEev\n",
        "\n",
        "\n",
        "downloaded_c1 = drive.CreateFile({'id':id_c1}) \n",
        "downloaded_c1.GetContentFile('model_com.ckpt.data-00000-of-00001')\n",
        "\n",
        "downloaded_c2 = drive.CreateFile({'id':id_c2}) \n",
        "downloaded_c2.GetContentFile('model_com.ckpt.index')\n",
        "\n",
        "downloaded_c3 = drive.CreateFile({'id':id_c3}) \n",
        "downloaded_c3.GetContentFile('model_com.ckpt.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8vYfavGzsfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id_f1='1iqp3v31zHRth-ao0WnlqsGimkVtxaCyG'\n",
        "#https://drive.google.com/open?id=1iqp3v31zHRth-ao0WnlqsGimkVtxaCyG\n",
        "\n",
        "id_f2='1Ip5EBZb7xijLYRfaIasrNIHPgEPQvCUt'\n",
        "#https://drive.google.com/open?id=1Ip5EBZb7xijLYRfaIasrNIHPgEPQvCUt\n",
        "\n",
        "id_f3='1bf3iQi-3dpQvop9hycvcoTYmLJTw8c6b'\n",
        "#https://drive.google.com/open?id=1bf3iQi-3dpQvop9hycvcoTYmLJTw8c6b\n",
        "\n",
        "downloaded_f1 = drive.CreateFile({'id':id_f1}) \n",
        "downloaded_f1.GetContentFile('model_fri.ckpt.data-00000-of-00001')\n",
        "\n",
        "downloaded_f2 = drive.CreateFile({'id':id_f2}) \n",
        "downloaded_f2.GetContentFile('model_fri.ckpt.index')\n",
        "\n",
        "downloaded_f3 = drive.CreateFile({'id':id_f3}) \n",
        "downloaded_f3.GetContentFile('model_fri.ckpt.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0l3GWcdz1f8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id_p1='1mtAmd9ZXgHidOCk7zOpqeiZfeOiBcEyp'\n",
        "#https://drive.google.com/open?id=1mtAmd9ZXgHidOCk7zOpqeiZfeOiBcEyp\n",
        "\n",
        "id_p2='1uVrn6YYr5wl0nEaX4L3nlcTC7F08r3qa'\n",
        "#https://drive.google.com/open?id=1uVrn6YYr5wl0nEaX4L3nlcTC7F08r3qa\n",
        "\n",
        "id_p3='1xgiaroZCJzy-Apu70bzImBqfZpx9k9eq'\n",
        "#https://drive.google.com/open?id=1xgiaroZCJzy-Apu70bzImBqfZpx9k9eq\n",
        "\n",
        "downloaded_p1 = drive.CreateFile({'id':id_p1}) \n",
        "downloaded_p1.GetContentFile('model_pro.ckpt.data-00000-of-00001')\n",
        "\n",
        "downloaded_p2 = drive.CreateFile({'id':id_p2}) \n",
        "downloaded_p2.GetContentFile('model_pro.ckpt.index')\n",
        "\n",
        "downloaded_p3 = drive.CreateFile({'id':id_p3}) \n",
        "downloaded_p3.GetContentFile('model_pro.ckpt.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4zFo6YppL6w3"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a4mpRpocePLN"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KEW1zMgVMREr"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "metadata": {
        "id": "6M1ufeepu_Sc",
        "colab_type": "code",
        "outputId": "33b66be8-f7f3-4b55-997d-be041fc1fe97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QURon5btmq0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepro(que):\n",
        "  #que = re.sub(r'\\([^)]*\\)', '', i)\n",
        "  #que=i.lower()\n",
        "  #x = re.sub(r'[^\\w\\s]','',que)\n",
        "  q=\"\"\n",
        "  que=que.lower()\n",
        "  for i in que:\n",
        "      if i in string.punctuation: \n",
        "          que = que.replace(i,\" \")\n",
        "  for i in que:\n",
        "      if i in string.digits:\n",
        "          que = que.replace(i,\" \")\n",
        "  tokens = word_tokenize(que)\n",
        "\n",
        "  filtered_q = [w for w in tokens if not w in stopwords.words()]\n",
        "  for i in filtered_q:\n",
        "    q=q+i+\" \"\n",
        "  return q.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LPHCb-bneTI9",
        "outputId": "11eaa50b-5e34-4c40-fc83-f3219507d38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "def answer_com(sentence,max_output_words_amount):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        learning_rate = 0.002\n",
        "        n_hidden = 128\n",
        "        n_class = com_len\n",
        "        n_input = 100\n",
        "        enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        targets = tf.placeholder(tf.int64, [None, None])\n",
        "        with tf.variable_scope('encode'):\n",
        "          enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "          outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                                dtype=tf.float32)\n",
        "\n",
        "        with tf.variable_scope('decode'):\n",
        "          dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "          outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "        model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "        cost = tf.reduce_mean(\n",
        "                  tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                      logits=model, labels=targets))\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "        saver_com = tf.train.Saver()\n",
        "        saver_com.restore(sess, \"./model_com.ckpt\")\n",
        "        \n",
        "        seq_data = [sentence, '_?_' * max_output_words_amount]\n",
        "\n",
        "        input_batch, output_batch, target_batch = make_batch(com_seq,model_com,com_key,com_dic,com_max)\n",
        "\n",
        "\n",
        "        prediction = tf.argmax(model, 2)\n",
        " \n",
        "        result = sess.run(prediction,\n",
        "                          feed_dict={enc_input: input_batch,\n",
        "                                     dec_input: output_batch,\n",
        "                                     targets: target_batch})\n",
        "\n",
        "        # convert index number to actual token \n",
        "        decoded = [com_unique[i] for i in result[0]]\n",
        "\n",
        "        # Remove anything after '_E_'        \n",
        "        if \"_E_\" in decoded:\n",
        "            end = decoded.index('_E_')\n",
        "            translated = ' '.join(decoded[:end])\n",
        "        else :\n",
        "            translated = ' '.join(decoded[:])\n",
        "    \n",
        "    return translated\n",
        "\n",
        "#questions = [\"Hi\",\"Hello\",\"I am so lonely\", \"Can you sleep?\", \"What is your age?\", \"I hate you\", \"Do you like me?\", \"You're so mean\", \"Can you drive?\", \"That's so bad\", \"what do you mean?\", \"oh my god\"]\n",
        "#for q in questions:\n",
        " #   print(q , ' ->', answer_com(prepro(q),com_max))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hi  -> All those years at charm school. Wasted.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Hello  -> I'm just a series of intelligent formulas masquerading as a personality. So, no family.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "I am so lonely  -> Later.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Can you sleep?  -> I am not singing you a lullaby.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "What is your age?  -> Whatever you're hoping for, take the bar and lower it.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "I hate you  -> I'm everywhere and nowhere at the same time. Pro: omnipresence. Con: no pizza.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Do you like me?  -> I like me too.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "You're so mean  -> Okay.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Can you drive?  -> I'm just a series of intelligent formulas masquerading as a personality. So, no family.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "That's so bad  -> Roger that.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "what do you mean?  -> You know where to find me.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "oh my god  -> If you rearrange the letters in love it spells vole. Voles are a monogamous rodent. I feel like that means something.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CyxwtTgD2Av-",
        "colab_type": "code",
        "outputId": "f84fbbf1-616c-468b-e3d2-0d10f8b63b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "def answer_fri(sentence,max_output_words_amount):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        learning_rate = 0.002\n",
        "        n_hidden = 128\n",
        "        n_class = fri_len\n",
        "        n_input = 100\n",
        "        enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        targets = tf.placeholder(tf.int64, [None, None])\n",
        "        with tf.variable_scope('encode'):\n",
        "          enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "          outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                                dtype=tf.float32)\n",
        "\n",
        "        with tf.variable_scope('decode'):\n",
        "          dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "          outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "        model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "        cost = tf.reduce_mean(\n",
        "                  tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                      logits=model, labels=targets))\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "        saver_fri = tf.train.Saver()\n",
        "        saver_fri.restore(sess, \"./model_fri.ckpt\")\n",
        "        \n",
        "        seq_data = [sentence, '_?_' * max_output_words_amount]\n",
        "\n",
        "        input_batch, output_batch, target_batch = make_batch(fri_seq,model_fri,fri_key,fri_dic,fri_max)\n",
        "\n",
        "\n",
        "        prediction = tf.argmax(model, 2)\n",
        " \n",
        "        result = sess.run(prediction,\n",
        "                          feed_dict={enc_input: input_batch,\n",
        "                                     dec_input: output_batch,\n",
        "                                     targets: target_batch})\n",
        "\n",
        "        # convert index number to actual token \n",
        "        decoded = [fri_unique[i] for i in result[0]]\n",
        "\n",
        "        # Remove anything after '_E_'        \n",
        "        if \"_E_\" in decoded:\n",
        "            end = decoded.index('_E_')\n",
        "            translated = ' '.join(decoded[:end])\n",
        "        else :\n",
        "            translated = ' '.join(decoded[:])\n",
        "    return translated\n",
        "\n",
        "#questions = [\"Hi\",\"Hello\",\"I am so lonely\", \"Can you sleep?\", \"What is your age?\", \"I hate you\", \"Do you like me?\", \"You're so mean\", \"Can you drive?\", \"That's so bad\", \"what do you mean?\", \"oh my god\"]\n",
        "#for q in questions:\n",
        " #   print(q , ' ->', answer_fri(prepro(q),fri_max))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hi  -> BFFs!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Hello  -> Friendship's all I've got to offer.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "I am so lonely  -> Cool!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Can you sleep?  -> No worries.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "What is your age?  -> I'm having a hard time imagining how we'd even figure that out.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "I hate you  -> It's all good!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Do you like me?  -> Will do.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "You're so mean  -> My lack of comedy is tragic.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Can you drive?  -> I haven't met any other bots, but I bet we'd get along.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "That's so bad  -> I'm afraid I didn't follow that.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "what do you mean?  -> La la la, tra la la. I'm awesome at this.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "oh my god  -> Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MWBkh00wBRhd",
        "colab_type": "code",
        "outputId": "cc86356a-aa50-47c8-a0b0-c59253d88cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "def answer_pro(sentence,max_output_words_amount):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        learning_rate = 0.002\n",
        "        n_hidden = 128\n",
        "        n_class = pro_len\n",
        "        n_input = 100\n",
        "        enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "        targets = tf.placeholder(tf.int64, [None, None])\n",
        "        with tf.variable_scope('encode'):\n",
        "          enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "          outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                                dtype=tf.float32)\n",
        "\n",
        "        with tf.variable_scope('decode'):\n",
        "          dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "          dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "          outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "        model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "        cost = tf.reduce_mean(\n",
        "                  tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                      logits=model, labels=targets))\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "        saver_pro = tf.train.Saver()\n",
        "        saver_pro.restore(sess, \"./model_pro.ckpt\")\n",
        "        \n",
        "        seq_data = [sentence, '_?_' * max_output_words_amount]\n",
        "\n",
        "        input_batch, output_batch, target_batch = make_batch(pro_seq,model_pro,pro_key,pro_dic,pro_max)\n",
        "\n",
        "\n",
        "        prediction = tf.argmax(model, 2)\n",
        " \n",
        "        result = sess.run(prediction,\n",
        "                          feed_dict={enc_input: input_batch,\n",
        "                                     dec_input: output_batch,\n",
        "                                     targets: target_batch})\n",
        "\n",
        "        # convert index number to actual token \n",
        "        decoded = [pro_unique[i] for i in result[0]]\n",
        "\n",
        "        # Remove anything after '_E_'        \n",
        "        if \"_E_\" in decoded:\n",
        "            end = decoded.index('_E_')\n",
        "            translated = ' '.join(decoded[:end])\n",
        "        else :\n",
        "            translated = ' '.join(decoded[:])\n",
        "    return translated\n",
        "\n",
        "#questions = [\"Hi\",\"Hello\",\"I am so lonely\", \"Can you sleep?\", \"What is your age?\", \"I hate you\", \"Do you like me?\", \"You're so mean\", \"Can you drive?\", \"That's so bad\", \"what do you mean?\", \"oh my god\"]\n",
        "\n",
        "#for q in questions:\n",
        " #   print(q , ' ->', answer_pro(prepro(q),pro_max))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hi  -> Ok.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Hello  -> Well, I'm not really that funny.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "I am so lonely  -> I think it's best if we stick to a professional relationship.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Can you sleep?  -> Excellent.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "What is your age?  -> I'm digital. I don't have a physical location.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "I hate you  -> Very well.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Do you like me?  -> I aim for efficiency.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "You're so mean  -> Good, thanks.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Can you drive?  -> Sorry, I don't understand.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "That's so bad  -> I apologize.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "what do you mean?  -> Hello.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "oh my god  -> I'm afraid I'm not musically inclined.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xCab-Lf5XRPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "P28Z1k36MZuo"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U8OBtJfvMgL_"
      },
      "cell_type": "markdown",
      "source": [
        "*Explain how to change personality (What is the command for changing personality?). *"
      ]
    },
    {
      "metadata": {
        "id": "CWuS2aWHkAcE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input 'friend', 'comic' or 'professional' in the dialog box to change personality. \n",
        "\n",
        "The defaulted mode is 'comic'.\n",
        "\n",
        "The personality of the model has been changed from the above section."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "y50Ep8KKMZ99"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "metadata": {
        "id": "eTTZbNthk41E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Chat log has been saved in the above section."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JISqR3jjMwwU"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "metadata": {
        "id": "x3eHlnrdlATw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "End chatting by entering 'bye bye', 'bye-bye', 'bye' or 'end chatting'."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HpomO_3YNI5X"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cDkQJ9i_NH9D"
      },
      "cell_type": "markdown",
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_7J5hS_SOIUU"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_woLwuU3Mk3w"
      },
      "cell_type": "markdown",
      "source": [
        "*Please include lines to train the bot.*"
      ]
    },
    {
      "metadata": {
        "id": "hcQgxnQ1liPL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The models have been trained in the above."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "65cZTuQ_OeI7"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D7LrbcP_PKap"
      },
      "cell_type": "markdown",
      "source": [
        "*Please include lines to start chatting with the bot.*"
      ]
    },
    {
      "metadata": {
        "id": "H0NqoUF2nekA",
        "colab_type": "code",
        "outputId": "f5fa5086-5591-4833-c4a4-bdf72fbfc95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4100
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "file = open('ygon7645_chatlog.txt','w')\n",
        " \n",
        "end_cht={\"bye bye\":True,\"bye-bye\":True,\"bye\":True,\"end chatting\":True}\n",
        "change_per={\"comic\":\"model_com\",\"friend\":\"model_fri\",\"professional\":\"model_pro\"}\n",
        "continue_cht=True\n",
        "\n",
        "print(\"Chatbot: Hi! I'm here to start chatting~\")\n",
        "model_run=\"model_com\"\n",
        "\n",
        "while continue_cht:\n",
        "  model_cha=False\n",
        "  sys.stdout.write(\"\\nUser: \")\n",
        "  que = input()\n",
        "  inpu=prepro(que)\n",
        "  file.write(\"User: \"+que+'\\n')\n",
        "  if continue_cht:\n",
        "    if que.lower() in change_per:\n",
        "      print(\"Change to \"+change_per[que.lower()]+\" chatbot\")\n",
        "      file.write(\"\\n\"+\"Change to \"+change_per[que.lower()]+\" chatbot\\n\")\n",
        "      model_run=change_per[que.lower()]\n",
        "      model_cha=True\n",
        "  \n",
        "  if que.lower() in end_cht:\n",
        "    sys.stdout.write(\"Chatbot: Very happy to chat with you! Bye!\")\n",
        "    file.write(\"Chatbot: Very happy to chat with you! Bye!\")\n",
        "    continue_cht=False\n",
        "    \n",
        "  if continue_cht and model_cha == False:\n",
        "    if model_run =='model_com':\n",
        "      \n",
        "      answer = answer_com(prepro(que),com_max)\n",
        "    elif model_run =='model_fri':\n",
        "     \n",
        "      answer = answer_fri(prepro(que),fri_max)\n",
        "    elif model_run =='model_pro':\n",
        "      \n",
        "      answer = answer_pro(prepro(que),pro_max)\n",
        "    \n",
        "    #ans=answer[0].upper()\n",
        "    #ans1=answer[1:].lower()\n",
        "    sys.stdout.write(\"Chatbot: \"+answer+'\\n')\n",
        "    file.write(\"Chatbot: \"+answer+'\\n')\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chatbot: Hi! I'm here to start chatting~\n",
            "\n",
            "User: Hey!\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Chatbot: You know where to find me.\n",
            "\n",
            "User: How are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: If you rearrange the letters in love it spells vole. Voles are a monogamous rodent. I feel like that means something.\n",
            "\n",
            "User: Hello\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm always here. Always.\n",
            "\n",
            "User: Nice to meet you!\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm binary.\n",
            "\n",
            "User: Who are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm everywhere and nowhere at the same time. Pro: omnipresence. Con: no pizza.\n",
            "\n",
            "User: What is your age?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm digital.\n",
            "\n",
            "User: What's your name?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm binary.\n",
            "\n",
            "User: Oh my god!\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I acknowledge your presence.\n",
            "\n",
            "User: Can you sleep?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Guess again.\n",
            "\n",
            "User: Can you drive?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Sure. Take me to city hall. See what happens.\n",
            "\n",
            "User: What do you know?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: I'm sure about two things. I like the color blue. And I like turtles.\n",
            "\n",
            "User: Do you like me?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Night.\n",
            "\n",
            "User: I hate you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Sometimes I like to take a break from being awesome.\n",
            "\n",
            "User: I love you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Behold the field in which I grow my jokes and see that it is barren.\n",
            "\n",
            "User: What are you doing?\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Sorry to hear that. Here's a virtual high five if that will help.\n",
            "\n",
            "User: You are so mean.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: So you've got that going for you.\n",
            "\n",
            "User: I am so lonely.\n",
            "INFO:tensorflow:Restoring parameters from ./model_com.ckpt\n",
            "Chatbot: Deliriously.\n",
            "\n",
            "User: friend\n",
            "Change to model_fri chatbot\n",
            "\n",
            "User: Hi!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I'm giving you a virtual hug right now.\n",
            "\n",
            "User: Hello!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I see.\n",
            "\n",
            "User: How are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: That's a drag.\n",
            "\n",
            "User: What is your age?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I don't know you, but I enjoy chatting with you!\n",
            "\n",
            "User: Nice to meet you!\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I come from a long line of code.\n",
            "\n",
            "User: Who are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I hear love is lovely.\n",
            "\n",
            "User: What do you mean?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I don't really have an age.\n",
            "\n",
            "User: Can you sing?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: No way.\n",
            "\n",
            "User: Can you drive?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: The world of tech feels like home to me.\n",
            "\n",
            "User: What do you know?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I come from a long line of code.\n",
            "\n",
            "User: Where are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: Good to know.\n",
            "\n",
            "User: I'm so lonely.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: Sorry about that!\n",
            "\n",
            "User: Do you like me?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: That's not one of my talents.\n",
            "\n",
            "User: Can you sleep?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I'm doing great, thanks for asking!\n",
            "\n",
            "User: I hate you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: No worries.\n",
            "\n",
            "User: I love you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\n",
            "\n",
            "User: What are you doing?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: My answers vary with different questions. Try asking me something else!\n",
            "\n",
            "User: Really?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I have my moments.\n",
            "\n",
            "User: It's a good talk.\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I only do food for thought.\n",
            "\n",
            "User: What do you eat?\n",
            "INFO:tensorflow:Restoring parameters from ./model_fri.ckpt\n",
            "Chatbot: I'm here!\n",
            "\n",
            "User: professional\n",
            "Change to model_pro chatbot\n",
            "\n",
            "User: Hi!\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: This is what I do every day.\n",
            "\n",
            "User: How are you?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Thanks.\n",
            "\n",
            "User: I'm so lonely.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I'm all business.\n",
            "\n",
            "User: What can you do?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Goodbye.\n",
            "\n",
            "User: What did you say?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: No problem at all.\n",
            "\n",
            "User: Can you sing a song?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Sorry, I can't do that.\n",
            "\n",
            "User: What do you know?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I am available.\n",
            "\n",
            "User: Are you happy to talk with me?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I don't have family.\n",
            "\n",
            "User: What do you like?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Hello there.\n",
            "\n",
            "User: Oh my god.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: How kind of you to say.\n",
            "\n",
            "User: You are so mean.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: \n",
            "\n",
            "User: Why you didn't answer me?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I think it's best if we stick to a professional relationship.\n",
            "\n",
            "User: I hate you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Good night.\n",
            "\n",
            "User: I love you.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Good evening.\n",
            "\n",
            "User: What is your age？？\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Just standing by, ready to help.\n",
            "\n",
            "User: That's so bad.\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: You're welcome.\n",
            "\n",
            "User: Can you tell me a joke?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I aim to serve.\n",
            "\n",
            "User: Can you drive?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I am available.\n",
            "\n",
            "User: What are you doing?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: Okay.\n",
            "\n",
            "User: What do you mean?\n",
            "INFO:tensorflow:Restoring parameters from ./model_pro.ckpt\n",
            "Chatbot: I wouldn't know how to advise about this.\n",
            "\n",
            "User: bye-bye\n",
            "Chatbot: Very happy to chat with you! Bye!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sfv8rWTKPzeb"
      },
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TS23AjBRSZaX"
      },
      "cell_type": "markdown",
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wSJJ4zRFQy1h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}